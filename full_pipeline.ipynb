{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62078559",
   "metadata": {},
   "source": [
    "**Loading necessary libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14e349cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "from langgraph.graph import START, END, StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Image, display\n",
    "from typing import Literal\n",
    "import os\n",
    "\n",
    "print(\"✅ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81e63704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ API key loaded\n"
     ]
    }
   ],
   "source": [
    "# Load API key\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_api_key:\n",
    "    raise ValueError(\"OPENAI_API_KEY not found! Please set it in your .env file.\")\n",
    "\n",
    "print(\"✅ API key loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4d90d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLM initialized: gpt-4o-mini\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    temperature=0.5,\n",
    "    api_key=openai_api_key\n",
    ")\n",
    "\n",
    "print(f\"✅ LLM initialized: {llm.model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8a52f",
   "metadata": {},
   "source": [
    "**Loading and Processing Document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1c543a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 ✅ Loaded 4 pages from Amendment_of_the_National_Law_on_NELFUND.pdf\n",
      "\n",
      "2 ✅ Loaded 11 pages from Explanatory_Memorandum_For_Students-Loans_Act_2023.pdf\n",
      "\n",
      "3 ✅ Loaded 4 pages from FAQs_On_NELFUND_Student_Loan.pdf\n",
      "\n",
      "4 ✅ Loaded 14 pages from Guidelines_For_Nelfund_Loan.pdf\n",
      "\n",
      "5 ✅ Loaded 4 pages from Terms&Conditions_of_Students_Loan.pdf\n",
      "\n"
     ]
    }
   ],
   "source": [
    "document_path = r\"agent\\data\"\n",
    "document_list = os.listdir(document_path)\n",
    "\n",
    "for i, file in enumerate(document_list, start=1):\n",
    "    file_path = os.path.join(document_path, file)\n",
    "    \n",
    "    # Check if file exists\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"⚠️ File not found: {file_path}\")\n",
    "        print(\"Please update the file_path variable with your PDF file.\")\n",
    "    else:\n",
    "        # Load the PDF\n",
    "        loader = PyPDFLoader(file_path)\n",
    "        pages = []\n",
    "        \n",
    "        # Load pages (async loading)\n",
    "        async for page in loader.alazy_load():\n",
    "            pages.append(page)\n",
    "        \n",
    "        print(f\"{i} ✅ Loaded {len(pages)} pages from {file}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42656501",
   "metadata": {},
   "source": [
    "**Split into Chunks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7927b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created 7 chunks\n",
      "\n",
      "Sample chunk:\n",
      "Terms  &  Conditions  of  Students  Loan   I,  the  student  whose  personal  information  are  contained  in  this  \n",
      "Students’\n",
      " \n",
      "Loan\n",
      " \n",
      "Account,\n",
      " \n",
      "hereby\n",
      " \n",
      "agree\n",
      " \n",
      "to\n",
      " \n",
      "take\n",
      " \n",
      "the\n",
      " \n",
      "loan\n",
      " \n",
      "subject\n",
      " \n",
      "...\n"
     ]
    }
   ],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,      # Characters per chunk\n",
    "    chunk_overlap=100     # Overlap to preserve context\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "doc_splits = text_splitter.split_documents(pages)\n",
    "\n",
    "print(f\"✅ Created {len(doc_splits)} chunks\")\n",
    "print(f\"\\nSample chunk:\")\n",
    "print(f\"{doc_splits[0].page_content[:200]}...\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
